{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e46813",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import functions\n",
    "\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "np.set_printoptions(suppress=True) # Supress scientific notation when printing\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "import seaborn as sns\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import re # Regular expressions\n",
    "import networkx as nx # Package for graph represenations \n",
    "from datetime import datetime, time \n",
    "import pygraphviz as gv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ede61981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data from Rat5 on 2021-06-29 (20210629_Rat5.txt):\n",
      "47 trials found initially\n",
      "\n",
      "18 duplicate trials found\n",
      "\n",
      "Trial 1\n",
      "Trial 1\n",
      "Trial 1\n",
      "Trial 2\n",
      "Trial 3\n",
      "Trial 4\n",
      "Trial 5\n",
      "Trial 6\n",
      "Trial 7\n",
      "Trial 8\n",
      "Trial 9\n",
      "Trial 10\n",
      "Trial 11\n",
      "Trial 12\n",
      "Trial 13\n",
      "Trial 14\n",
      "Trial 14\n",
      "Trial 28\n",
      "1147 duplicate lines found\n",
      "\n",
      "29 unique trials left after duplicate removal\n",
      "\n",
      "Final number of trials loaded: 28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# def import_file(rat_filename, file_path = '../data/raw/', path2data = '../data/')\n",
    "# \"\"\"\n",
    "#  Reads Rat HexMaze behavioural data from experiment logs.\n",
    "#  Performs basic cleaning and sanity checks.\n",
    "#  Re-organises in a pandas-friendly format, and returns pandas dataframe.\n",
    "#  Import logs are also generated and saved at path2results path.\n",
    " \n",
    "#  Parameters\n",
    "#  ----------\n",
    "#  arg1 : str\n",
    "#      Filename of txt data with experimental logs for single rat\n",
    "#  arg2 : str, optional\n",
    "#      Path to where rat_filename is stored\n",
    "#  arg3 : str, optional\n",
    "#      Path to directory where import logs and re-organized csv should be saved\n",
    "\n",
    "# Returns\n",
    "# -------\n",
    "# pandas dataframe with clean data\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "## Set file names and paths\n",
    "# Raw data directory\n",
    "path2data = '../data/'\n",
    "# Results directory\n",
    "path2results = '../results/'\n",
    "# Raw data file\n",
    "rat_filename = '20210629_Rat5.txt'\n",
    "# I usually have a directory for each project, with subfolders at least for\n",
    "# for data (data), scripts (src), plots and other results (results), hence the paths specified below\n",
    "\n",
    "\n",
    "# Set name for data import log file\n",
    "today = datetime.now()\n",
    "log_filename = 'import_log_' + today.strftime(\"%Y%m%d\") + '.txt'\n",
    "# Create/Overwrite log file (A new logfile will be created each day\n",
    "# the script is run)   \n",
    "with open(path2results + log_filename, 'w') as log_f:\n",
    "    log_f.write('Import date: ' + today.strftime(\"%d-%m-%y\") + '\\n')\n",
    "    log_f.write(list_all[0])\n",
    "\n",
    "# Extract exp. date and Rat Id from file name (format: YYYYMMDD_RatX.txt)\n",
    "filename_info = rat_filename.split('_')\n",
    "exp_date = pd.to_datetime(filename_info[0]).date()\n",
    "rat_id = filename_info[1].split('.')[0]\n",
    "\n",
    "info_msg = 'Data from ' + rat_id + ' on ' + exp_date.strftime('%Y-%m-%d') + ' ('+ rat_filename + ')'\n",
    "print('Loading ' + info_msg + ':')\n",
    "\n",
    "\n",
    "# Open logfile again to start appending import logs\n",
    "log_f = open(path2results + log_filename, 'a')\n",
    "log_f.write('\\n\\n=========================================================================\\n')\n",
    "log_f.write(info_msg)\n",
    "log_f.write('\\n=========================================================================\\n')\n",
    "\n",
    "# Extract fileinfo to list_all\n",
    "with open(path2data + rat_filename, 'r') as f:\n",
    "    # Load all lines from file\n",
    "    list_all = [x for k,x in enumerate(f.readlines())]\n",
    "        \n",
    "# Locate trial line boundaries and first trial line\n",
    "list_tr_bb = [n for n, x in enumerate(list_all) if re.match(r'^Summary Trial', x)]\n",
    "trial_headers = [x for n, x in enumerate(list_all) if re.match(r'^Summary Trial', x)]\n",
    "\n",
    "# Add a final entry to list_tr_bb to mark end boundary of last trial\n",
    "list_tr_bb.append(len(list_all))\n",
    "\n",
    "info_msg = str(len(list_tr_bb)) + ' trials found initially\\n'\n",
    "log_f.write(info_msg)\n",
    "print(info_msg)\n",
    "\n",
    "\n",
    "### DATA CLENANING\n",
    "# Data files contain duplicate lines and/or duplicate trials\n",
    "# Find and remove them\n",
    "\n",
    "# Look for duplicate trials to write info to log\n",
    "dupe_trials = [x for n, x in enumerate(trial_headers) if x in trial_headers[:n]] # Get duplicate trial headers\n",
    "\n",
    "info_msg = str(len(dupe_trials)) + ' duplicate trials found\\n'\n",
    "\n",
    "log_f.write(info_msg)\n",
    "[log_f.write('Trial ' + x.split()[2]) for x in dupe_trials]\n",
    "print(info_msg)\n",
    "[print('Trial ' + x.split()[2]) for x in dupe_trials]\n",
    "\n",
    "\n",
    "# Look for duplicate lines in general and remove them from list_all\n",
    "dupe_lines = [n for n, x in enumerate(list_all) if x in list_all[:n]]\n",
    "\n",
    "# Remove duplicate lines from list_all\n",
    "info_msg = str(len(dupe_lines)) + ' duplicate lines found\\n'\n",
    "print(info_msg)\n",
    "log_f.write(info_msg)\n",
    "for x in sorted(dupe_lines, reverse=True):\n",
    "    del list_all[x]\n",
    "\n",
    "# Update trial boundaries after deletions\n",
    "list_tr_bb = [n for n, x in enumerate(list_all) if re.match(r'^Summary Trial', x)]\n",
    "list_tr_bb.append(len(list_all))\n",
    "\n",
    "# Strip extraneous strings from file lines, and read data within each line into list of lists of strings\n",
    "list_sess = []\n",
    "for l,tr in enumerate(list_tr_bb[:-1]): \n",
    "    # Find all lines starting with ( between trial boundaries\n",
    "    list_sing = [x for x in list_all[list_tr_bb[l]:list_tr_bb[l+1]] if x[0]=='('] # Lines starting with ( \n",
    "    # Remove parenthesis, commas and '' from string\n",
    "    list_sing = [x.replace(\"(\", \"\") for x in list_sing]\n",
    "    list_sing = [x.replace(\")\", \"\") for x in list_sing]\n",
    "    list_sing = [x.replace(\",\", \"\") for x in list_sing]\n",
    "    list_sing = [x.replace(\"'\", \"\") for x in list_sing]\n",
    "    list_sing = [x.replace(\"\\n\", \"\") for x in list_sing]\n",
    "    # Now separate data in each line within the trial\n",
    "    list_sing = [x.split(sep=' ') for x in list_sing]\n",
    "    list_sess.append(list_sing)\n",
    "    \n",
    "info_msg = 'Final number of trials loaded: ' + str(len(list_sess)) + '\\n'\n",
    "print(info_msg)\n",
    "log_f.write(info_msg)\n",
    "\n",
    "log_f.close()\n",
    "\n",
    "## TRANSFORM TO 'LONG' FORMAT AND SAVE IN PANDAS DATA FRAME\n",
    "# Transform current 'wide' data format into 'long' format, and store as a pandas \n",
    "# dataframe.\n",
    "# Edge trajectory data will be re-ordered into a successive list of nodes and times.\n",
    "# Additional data, such as seconds, distance, and speed refers to the trajectory from\n",
    "# node[i-1] to node[i], being zero for the inital row.\n",
    "\n",
    "# First re-order and store as single lists for each variable\n",
    "# Variables with _ff suffix  (\"from file\") can be computed from other primary variables in the file. \n",
    "# Suffix is included in case these vars need to be re-computed \n",
    "\n",
    "size = sum([len(x) for x in list_sess]) + len(list_sess) # List size: total no. of lines (across all trials), \n",
    "                                                # with one additional line per trial (25 trials in total) to flatten structure\n",
    "trial_no = [None]*size \n",
    "distance = [None]*size \n",
    "seconds_ff = [None]*size # seconds from file \n",
    "nodes = [None]*size #np.zeros(size, dtype=int) # nodes the rat passes through (flat)\n",
    "speed_ff = [None]*size #np.zeros(size)\n",
    "times = [None]*size #np.zeros(shape) # all timestamps (flat)\n",
    "rat_id_col = [rat_id]*size\n",
    "exp_date_coil = [exp_date]*size\n",
    "\n",
    "\n",
    "line_no = 0 # initialize counter\n",
    "\n",
    "for tr, trial in enumerate(list_sess):\n",
    "\n",
    "    # Add inital trial line for nodes and timestamps array with starting node (first value of first line in the trial)\n",
    "    trial_no[line_no] = tr+1\n",
    "    nodes[line_no] = trial[0][0] # First value from first row in that trial: start node\n",
    "    times[line_no] = trial[0][2] # Third value from first row in that trial: start time\n",
    "    # times[:, cum_line_no] = [float(x) for x in time_list]\n",
    "    # Initialize distance and seconds\n",
    "    distance[line_no] = 0\n",
    "    seconds_ff[line_no] = 0\n",
    "    speed_ff[line_no] = 0\n",
    "    line_no +=1\n",
    "    \n",
    "    for row in trial:\n",
    "        \n",
    "        trial_no[line_no] = tr+1\n",
    "        nodes[line_no] = row[1]\n",
    "        times[line_no] = row[3]\n",
    "        seconds_ff[line_no] = row[4]\n",
    "        distance[line_no] = row[5]\n",
    "        speed_ff[line_no] = row[6]     \n",
    "        line_no += 1\n",
    "\n",
    "\n",
    "## BUILD PD DATA FRAME\n",
    "\n",
    "data = pd.DataFrame(list(zip(rat_id_col, exp_date_col, trial_no, nodes, times, distance, seconds_ff, speed_ff)),\n",
    "                   columns = ['rat_id', 'date', 'trial_no', 'node', 'time_ff', 'distance', \\\n",
    "                              'seconds_ff', 'speed_ff']) \n",
    "\n",
    "return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8921c401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c7768c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc34a66b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
